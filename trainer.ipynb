{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['Italo', 'Ste', 'Gustavo']\n",
      "Number of images: 70\n",
      "Number of labels: 70\n",
      "Labels: [0 1 2]\n",
      "Label counts: [28 28 14]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class HandwritingDataset(Dataset):\n",
    "    def __init__(self, image_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.images = self.get_image_paths_and_labels()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path, label = self.images[idx]\n",
    "        image = Image.open(image_path).convert(\"L\")  # Convert to grayscale\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "    def get_image_paths_and_labels(self):\n",
    "        image_paths = []\n",
    "        labels = []\n",
    "        classes = os.listdir(self.image_dir)\n",
    "        for idx, class_name in enumerate(classes):\n",
    "            class_dir = os.path.join(self.image_dir, class_name)\n",
    "            for filename in os.listdir(class_dir):\n",
    "                if filename.endswith(\".png\"):  # Assuming your images are PNGs\n",
    "                    image_paths.append(os.path.join(class_dir, filename))\n",
    "                    labels.append(idx)\n",
    "        print(\"Classes:\", classes)\n",
    "        print(\"Number of images:\", len(image_paths))\n",
    "        print(\"Number of labels:\", len(labels))\n",
    "        print(\"Labels:\", np.unique(labels))\n",
    "        print(\"Label counts:\", np.bincount(labels))\n",
    "\n",
    "        # self.classes = classes\n",
    "        # self.labels = labels\n",
    "        self.labels_map = {i: label for i, label in enumerate(classes)}\n",
    "        return list(zip(image_paths, labels))\n",
    "\n",
    "\n",
    "# Define transforms\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((28, 28)),  # Adjust size as needed\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Initialize dataset\n",
    "dataset = HandwritingDataset(\"Imagens rotacionadas\", transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into a training set and a validation set\n",
    "val_ratio = 0.05  # 20% of the data will be used for validation\n",
    "batch_size = 32\n",
    "\n",
    "val_size = int(len(dataset) * val_ratio)\n",
    "train_size = len(dataset) - val_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create data loaders for the training set and the validation set\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, 3)  # TODO: 3 classes for person A, B, C\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 7 * 7)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, pool_size, fc1_out_features, fc2_out_features):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=pool_size, stride=pool_size)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels*2, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        self.fc1 = nn.Linear(out_channels*2 * 7 * 7, fc1_out_features)\n",
    "        self.fc2 = nn.Linear(fc1_out_features, fc2_out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, self.fc1.in_features)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "NVIDIA GeForce RTX 3050 Ti Laptop GPU\n",
      "_CudaDeviceProperties(name='NVIDIA GeForce RTX 3050 Ti Laptop GPU', major=8, minor=6, total_memory=3897MB, multi_processor_count=20)\n",
      "True\n",
      "18727424\n",
      "119537664\n"
     ]
    }
   ],
   "source": [
    "cuda_device = torch.cuda.current_device()\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(cuda_device))\n",
    "print(torch.cuda.get_device_properties(cuda_device))\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.memory_allocated())\n",
    "print(torch.cuda.memory_reserved())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, device, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "    accuracy = correct / len(data_loader.dataset)\n",
    "    return running_loss / len(data_loader), accuracy\n",
    "\n",
    "def train(train_loader: DataLoader, val_loader: DataLoader, model: nn.Module, num_epochs: int):\n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            # Move data to the GPU\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()  # Zero the gradients\n",
    "            outputs = model(images)  # Forward pass\n",
    "            loss = criterion(outputs, labels)  # Compute loss\n",
    "            loss.backward()  # Backward pass\n",
    "            optimizer.step()  # Update weights\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        val_loss, val_acc = evaluate(model, val_loader, device, criterion)\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Val Loss: {val_loss}, Val Acc: {val_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'in_channels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[148], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSimpleCNN\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpool_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfc1_out_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfc2_out_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabels_map\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# model = SimpleCNN().to(device)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m train(train_loader, val_loader, model, \u001b[38;5;241m10\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'in_channels'"
     ]
    }
   ],
   "source": [
    "model = SimpleCNN(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1, pool_size=2, fc1_out_features=128, fc2_out_features=len(dataset.labels_map)).to(device)\n",
    "# model = SimpleCNN().to(device)\n",
    "train(train_loader, val_loader, model, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, image):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image)\n",
    "        _, pred = torch.max(outputs, dim=1)\n",
    "    return pred.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gustavo1.jpeg --- Gustavo\n",
      "Gustavo2.jpeg --- Gustavo\n",
      "Italo2.jpeg --- Italo\n",
      "Ste2.jpeg --- Ste\n",
      "Italo1.jpeg --- Gustavo\n",
      "Ste1.jpeg --- Ste\n"
     ]
    }
   ],
   "source": [
    "for file in os.listdir(\"validar\"):\n",
    "    image_path = \"validar/\" + file\n",
    "    image = Image.open(image_path).convert('L')  # Convert to grayscale\n",
    "    image = transform(image).to(device)\n",
    "    prediction = predict(model, image)\n",
    "    print(f\"{file} --- {dataset.labels_map[prediction]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
